{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3afa8bf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ParallelEnv.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# ----- 3. Create parallel environments -----\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# mp_context = mp.get_context(\"spawn\")\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# envs = ParallelEnv(4, make_env, multiprocessing_context=mp_context)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m envs = \u001b[43mParallelEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# ----- 4. Define the actor-critic model -----\u001b[39;00m\n\u001b[32m     34\u001b[39m obs_shape = envs.observation_space.shape  \u001b[38;5;66;03m# (3, 64, 64)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ParallelEnv.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# ðŸš€ torch-ac minimal training example\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch_ac\n",
    "from torch_ac.model import ACModel\n",
    "from torch_ac.algos.ppo import PPOAlgo\n",
    "from torch_ac.utils.penv import ParallelEnv\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from torch_ac.utils.penv import ParallelEnv\n",
    "\n",
    "\n",
    "\n",
    "# ----- 1. Set config parameters -----\n",
    "ENV_NAME = \"MiniGrid-DoorKey-6x6-v0\"\n",
    "NUM_ENVS = 4\n",
    "FRAMES_PER_PROC = 128\n",
    "TOTAL_FRAMES = 100_000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----- 2. Define environment factory -----\n",
    "def make_env():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = ImgObsWrapper(env)  # Use RGB-only observations\n",
    "    return env\n",
    "\n",
    "# ----- 3. Create parallel environments -----\n",
    "# mp_context = mp.get_context(\"spawn\")\n",
    "# envs = ParallelEnv(4, make_env, multiprocessing_context=mp_context)\n",
    "envs = ParallelEnv(4, make_env)\n",
    "\n",
    "# ----- 4. Define the actor-critic model -----\n",
    "obs_shape = envs.observation_space.shape  # (3, 64, 64)\n",
    "n_actions = envs.action_space.n           # e.g., 7 discrete actions\n",
    "\n",
    "model = ACModel(obs_shape, n_actions)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ----- 5. Initialize PPO algorithm -----\n",
    "ppo = PPOAlgo(\n",
    "    envs=envs,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    frames_per_proc=FRAMES_PER_PROC,\n",
    "    discount=0.99,\n",
    "    lr=0.00025,\n",
    "    gae_lambda=0.95,\n",
    "    entropy_coef=0.01,\n",
    "    value_loss_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    recurrence=1,  # Set >1 for recurrent policy (LSTM)\n",
    "    clip_eps=0.2,\n",
    "    epochs=4,\n",
    "    batch_size=256,\n",
    "    preprocess_obss=None  # No preprocessing wrapper needed here\n",
    ")\n",
    "\n",
    "# ----- 6. Training loop -----\n",
    "num_frames = 0\n",
    "update = 0\n",
    "\n",
    "while num_frames < TOTAL_FRAMES:\n",
    "    # 6.1 Collect rollouts from envs\n",
    "    exps, logs1 = ppo.collect_experiences()\n",
    "\n",
    "    # 6.2 Update policy and value networks\n",
    "    logs2 = ppo.update_parameters(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    # 6.3 Print summary every few updates\n",
    "    if update % 1 == 0:\n",
    "        ret = logs[\"return_per_episode\"]\n",
    "        print(f\"Update {update} | Frames {num_frames} | Mean Return: {np.mean(ret):.2f} | Episodes: {len(ret)}\")\n",
    "\n",
    "print(\"âœ… Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2af4411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ParallelEnv signature: (envs)\n"
     ]
    }
   ],
   "source": [
    "from torch_ac.utils.penv import ParallelEnv\n",
    "import inspect\n",
    "\n",
    "print(\"âœ… ParallelEnv signature:\", inspect.signature(ParallelEnv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a6abb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee37a778",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ACModel' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m n_actions = env.action_space.n\n\u001b[32m     29\u001b[39m model = ACModel(obs_shape, n_actions)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(DEVICE)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# ----- 5. Create PPO algorithm -----\u001b[39;00m\n\u001b[32m     33\u001b[39m ppo = PPOAlgo(\n\u001b[32m     34\u001b[39m     envs=envs,\n\u001b[32m     35\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     preprocess_obss=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'ACModel' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from minigrid.wrappers import ImgObsWrapper\n",
    "from torch_ac.model import ACModel\n",
    "from torch_ac.algos.ppo import PPOAlgo\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ----- 1. Set config parameters -----\n",
    "ENV_NAME = \"MiniGrid-DoorKey-6x6-v0\"\n",
    "FRAMES_PER_PROC = 128       # How many steps per update\n",
    "TOTAL_FRAMES = 50000        # How long to train\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----- 2. Create a single wrapped environment -----\n",
    "def make_env():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = ImgObsWrapper(env)  # Use RGB image observations\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "# ----- 3. Wrap single env in a dummy list -----\n",
    "envs = [env]  # Acts like ParallelEnv but with just one env\n",
    "\n",
    "# ----- 4. Define the model -----\n",
    "obs_shape = env.observation_space.shape  # e.g., (3, 64, 64)\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "model = ACModel(obs_shape, n_actions)\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ----- 5. Create PPO algorithm -----\n",
    "ppo = PPOAlgo(\n",
    "    envs=envs,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    frames_per_proc=FRAMES_PER_PROC,\n",
    "    discount=0.99,\n",
    "    lr=0.00025,\n",
    "    gae_lambda=0.95,\n",
    "    entropy_coef=0.01,\n",
    "    value_loss_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    recurrence=1,\n",
    "    clip_eps=0.2,\n",
    "    epochs=4,\n",
    "    batch_size=64,  # Smaller batch since only 1 env\n",
    "    preprocess_obss=None\n",
    ")\n",
    "\n",
    "# ----- 6. Training loop -----\n",
    "num_frames = 0\n",
    "update = 0\n",
    "\n",
    "while num_frames < TOTAL_FRAMES:\n",
    "    exps, logs1 = ppo.collect_experiences()\n",
    "    logs2 = ppo.update_parameters(exps)\n",
    "    logs = {**logs1, **logs2}\n",
    "\n",
    "    num_frames += logs[\"num_frames\"]\n",
    "    update += 1\n",
    "\n",
    "    print(f\"Update {update} | Frames {num_frames} | Mean Return: {np.mean(logs['return_per_episode']):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cb02fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minigrid \n",
    "import gymnasium as gym\n",
    "env = gym.make(\"MiniGrid-DoorKey-5x5-v0\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLHF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
