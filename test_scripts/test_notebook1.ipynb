{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7bb149",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\u001b[34;43m__file__\u001b[39;49m), \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMinigrid\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mminigrid\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtest_envs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TestEnv\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMinigrid\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mminigrid\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmanual_control\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ManualControl\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\")))\n",
    "from Minigrid.minigrid.envs.test_envs import TestEnv\n",
    "from Minigrid.minigrid.manual_control import ManualControl\n",
    "\n",
    "# Instantiate your custom DoorKeyEnv\n",
    "env = TestEnv( render_mode=\"human\")\n",
    "\n",
    "# Start manual control interface\n",
    "manual = ManualControl(env, seed=42)\n",
    "manual.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74dfe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'register_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mminigrid\u001b[39;00m  \u001b[38;5;66;03m# Ensures the base package is imported\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mminigrid\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menvs\u001b[39;00m  \u001b[38;5;66;03m# Triggers registration of built-in envs\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregister_env\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# print(sorted(gym.envs.registry.keys()))\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAvailable environments:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'register_env'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid  # Ensures the base package is imported\n",
    "import minigrid.envs  # Triggers registration of built-in envs\n",
    "from register_env \n",
    "# print(sorted(gym.envs.registry.keys()))\n",
    "print(\"Available environments:\")\n",
    "for env_id in gym.envs.registry.keys():\n",
    "    if \"MiniGrid\" in env_id:\n",
    "        print(f\"âœ… {env_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada2bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from AUTOMATA.auto_funcs import dfa_T1, DFAMonitor\n",
    "state_list = list(dfa_T1.states)\n",
    "size = len(state_list)\n",
    "sorted_states = sorted(state_list)\n",
    "enumerate_states = enumerate(sorted_states)\n",
    "states_dict = {state: index for index, state in enumerate(sorted(state_list))}\n",
    "states_dict[\"q5\"]\n",
    "states_dict[\"new\"] = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c33283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy \n",
    "x = torch.tensor([3, 2, 5, 0]).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7130b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "# File: minigrid/minigrid_env.py\n",
    "from enum import IntEnum\n",
    "\n",
    "class Actions(IntEnum):\n",
    "    left = 0\n",
    "    right = 1\n",
    "    forward = 2\n",
    "    pickup = 3\n",
    "    drop = 4\n",
    "    toggle = 5\n",
    "    done = 6\n",
    "\n",
    "# Load list of dicts from JSON file\n",
    "# file_name = r\"C:\\Users\\nsmith3\\Documents\\GitHub\\temporal_RLHF\\test_results\\path_log_episode_192.json\"\n",
    "file_name = r\"C:\\Users\\nsmith3\\Documents\\GitHub\\temporal_RLHF\\test_results\\path_log_episode_28.json\"\n",
    "with open(file_name, \"r\") as f:\n",
    "    episode_data = json.load(f)\n",
    "\n",
    "def int2action(int_list):\n",
    "        # Turn left, turn right, move forward\n",
    "   action_dict = {\"left\":0, \n",
    "                  \"right\":1, \n",
    "                  \"forward\":2, \n",
    "                  \"pickup\":3, \n",
    "                  \"drop\":4, \n",
    "                  \"toggle\":5,\n",
    "                  \"done\":6}\n",
    "   action_list = [action_dict[action_int] for action_int in int_list]\n",
    "\n",
    "actions_list = [dict[\"action\"] for dict in episode_data]\n",
    "reward_list  = [dict[\"reward\"] for dict in episode_data]\n",
    "event_list   = [dict[\"event\"] for dict in episode_data]\n",
    "transition_indices_list =[ i for i, v in enumerate(reward_list) if v != 0]\n",
    "reward_sum   = np.sum(np.array(reward_list))\n",
    "reward_actions = [actions_list[i] for i in transition_indices_list]\n",
    "reward_action_names = [Actions(a).name for a in reward_actions]\n",
    "reward_events = [event_list[index] for index in transition_indices_list]\n",
    "reward_at_events = [reward_list[index] for index in transition_indices_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Hankel matrix computation\n",
      "End of Hankel matrix computation\n",
      "Start Building Automaton from Hankel matrix\n",
      "End of Automaton computation\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import splearn\n",
    "from splearn.datasets.base import load_data_sample\n",
    "from splearn.spectral import Spectral\n",
    "from AUTOMATA.auto_funcs import spwfa2WFA\n",
    "\n",
    "train_file = r\"C:\\Users\\nsmith3\\Documents\\GitHub\\temporal_RLHF\\scikit-splearn-1.2.1\\splearn\\tests\\datasets\\3.pautomac.train\"\n",
    "train = load_data_sample(train_file)\n",
    "\n",
    "est = Spectral()\n",
    "est.get_params()\n",
    "est.set_params(lrows=5, lcolumns =5, smooth_method=\"trigram\" ,\n",
    "version=\"factor\" )\n",
    "\n",
    "# Spectral(full_svd_calculation=False , lcolumns=5, lrows =5,\n",
    "# mode_quiet=False , partial=True , rank =5,\n",
    "# smooth_method=\"trigram\" , sparse=True ,\n",
    "# version= \"factor\" )\n",
    "data = train.data\n",
    "est.fit(data)\n",
    "auto = est.automaton\n",
    "WFA = spwfa2WFA(wfa=auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9612f8d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 5$"
      ],
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from AUTOMATA.auto_funcs import *\n",
    "simple_wfa = simple_wfa()\n",
    "test_word = [\"b\", \"b\", \"a\"]\n",
    "simple_wfa.weight( test_word )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e8a830c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'est' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m test_sample_tuple = (\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, test_sample_splearn)\n\u001b[32m     12\u001b[39m test_data_sample  = DataSample(test_sample_tuple)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m hankel_test = \u001b[43mest\u001b[49m.hankel\n\u001b[32m     15\u001b[39m auto = est.automaton\n\u001b[32m     16\u001b[39m data = train.data \n",
      "\u001b[31mNameError\u001b[39m: name 'est' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from splearn.datasets.base import DataSample\n",
    "from splearn.spectral import Spectral\n",
    "from splearn.datasets.data_sample import SplearnArray\n",
    "\n",
    "test_sample = np.array([\n",
    "                        [0, 1, 2, -1, -1], \n",
    "                        [1, 1, -1, -1, -1]\n",
    "                        ])\n",
    "test_sample_splearn = SplearnArray(test_sample)\n",
    "test_sample_tuple = (2, 3, test_sample_splearn)\n",
    "test_data_sample  = DataSample(test_sample_tuple)\n",
    "\n",
    "hankel_test = est.hankel\n",
    "auto = est.automaton\n",
    "data = train.data \n",
    "weights = est.predict_proba(data)\n",
    "d = data[0]\n",
    "lh = hankel_test.lhankel[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3037c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      6\u001b[39m train_data = {\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33ma\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m10.0\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maa\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m7.0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m2.0\u001b[39m\n\u001b[32m     12\u001b[39m }\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Step 2: Wrap the dictionary into a DataSample object\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_sample = \u001b[43mDataSample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Step 3: Train the WFA using Spectral learning\u001b[39;00m\n\u001b[32m     18\u001b[39m est = Spectral(rank=\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\users\\nsmith3\\documents\\github\\temporal_rlhf\\scikit-splearn-1.2.1\\splearn\\datasets\\data_sample.py:157\u001b[39m, in \u001b[36mDataSample.__init__\u001b[39m\u001b[34m(self, data, **kwargs)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m._data = SplearnArray(np.zeros((\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m)))\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = SplearnArray(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m, nbL=data[\u001b[32m0\u001b[39m], nbEx=data[\u001b[32m1\u001b[39m])\n\u001b[32m    158\u001b[39m \u001b[38;5;28msuper\u001b[39m(DataSample, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(kwargs)\n",
      "\u001b[31mKeyError\u001b[39m: 2"
     ]
    }
   ],
   "source": [
    "from splearn.datasets.base import DataSample\n",
    "from splearn.spectral import Spectral\n",
    "\n",
    "# Step 1: Create your training sample as a dictionary\n",
    "# Each key is a string (sequence), and the value is its frequency or weight\n",
    "train_data = {\n",
    "    'a': 10.0,\n",
    "    'aa': 7.0,\n",
    "    'aaa': 3.0,\n",
    "    'ab': 5.0,\n",
    "    'b': 2.0\n",
    "}\n",
    "\n",
    "# Step 2: Wrap the dictionary into a DataSample object\n",
    "train_sample = DataSample(data=train_data)\n",
    "\n",
    "# Step 3: Train the WFA using Spectral learning\n",
    "est = Spectral(rank=3)\n",
    "est.fit(train_sample)\n",
    "\n",
    "# Step 4: Access the learned automaton\n",
    "automaton = est.automaton_\n",
    "print(\"âœ… Automaton states:\", automaton.states())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f7fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8142064  0.58258038 0.52577209 0.47661861 0.19967711 0.8873032\n",
      " 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import choix\n",
    "import networkx as nx\n",
    "\n",
    "n_items = 7\n",
    "data = [\n",
    "    (1, 0), (0, 4), (3, 1),\n",
    "    (0, 2), (2, 4), (4, 3), \n",
    "    (5,6)\n",
    "]\n",
    "\n",
    "params = choix.ilsr_pairwise(n_items, data, alpha = 0.1)\n",
    "updated_params = params + np.abs( np.min( params ) )\n",
    "print(updated_params)\n",
    "rows,_ = np.eye(4).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a2e14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import numpy as np \n",
    "# File: minigrid/minigrid_env.py\n",
    "from enum import IntEnum\n",
    "\n",
    "class Actions(IntEnum):\n",
    "    left = 0\n",
    "    right = 1\n",
    "    forward = 2\n",
    "    pickup = 3\n",
    "    drop = 4\n",
    "    toggle = 5\n",
    "    done = 6\n",
    "\n",
    "r = Actions(1).name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b6641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 0], [3, 3], [3, 0, 1, 3, 3, 1, 2, 0, 0], [1, 0, 0, 2, 1, 1, 2, 3, 2], [2, 0, 1], [3, 0, 3, 1, 3, 0, 3, 3]]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "max_length = 10\n",
    "num_letters = 4\n",
    "num_samples = 6\n",
    "np.random.permutation( random.randint(0, num_letters) ).tolist()[:max_length] \n",
    "list = [ [np.random.randint(0, num_letters) for _ in np.arange( np.random.randint(0, max_length) )] for _ in np.arange(num_samples) ]\n",
    "print(list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLHF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
